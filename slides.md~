---
# try also 'default' to start simple
theme: seriph
# random image from a curated Unsplash collection by Anthony
# like them? see https://unsplash.com/collections/94734566/slidev
background: https://source.unsplash.com/collection/94734566/1920x1080
# apply any windi css classes to the current slide
class: 'text-center'
# https://sli.dev/custom/highlighters.html
highlighter: shiki
# show line numbers in code blocks
lineNumbers: false
# some information about the slides, markdown enabled
info: |
  ## Slidev Starter Template
  Presentation slides for developers.

  Learn more at [Sli.dev](https://sli.dev)
# persist drawings in exports and build
drawings:
  persist: false
# page transition
transition: slide-left
# use UnoCSS
css: unocss
---

# [LoRA: Low-Rank Adaptation of Large Language Models](https://openreview.net/forum?id=nZeVKeeFYf9)
  
[Edward J Hu](https://openreview.net/profile?id=~Edward_J_Hu1),
[yelong shen](https://openreview.net/profile?id=~yelong_shen1),
[Phillip Wallis](https://openreview.net/profile?id=~Phillip_Wallis1),
[Zeyuan Allen-Zhu](https://openreview.net/profile?id=~Zeyuan_Allen-Zhu1),  
[Yuanzhi Li](https://openreview.net/profile?id=~Yuanzhi_Li1),
[Shean Wang](https://openreview.net/profile?id=~Shean_Wang1),
[Lu Wang](https://openreview.net/profile?email=luw%40microsoft.com),
[Weizhu Chen](https://openreview.net/profile?id=~Weizhu_Chen1)  
  
Microsoft, ICLR 2022

---

# Introduction

Many applications in natural language processing rely on applying a large-scale, pre-trained language model to multiple downstream applications through fine-tuning.

However, the fine-tuned model according to different tasks has the same size parameters as the pre-trained model,
This requires a lot of space to store the models of these specialized tasks.

---

# Introduction (cont.)

## Existing Solution

Many sought to mitigate this by adapting only some parameters or learning external modules for new tasks.  

e.g., Bias-only Tuning, Prefix-tuninig, Adapter Layer

<!--In this way, in addition to the pre-trained model for each task, only a small number of task-specific parameters need to be stored and loaded, which greatly improves the operating efficiency during deployment.-->

<div class="grid grid-cols-2">


<img class="w-7/8" src="/assets/prefix-tuning.png" />

<img class="" src=https://user-images.githubusercontent.com/26186289/236157447-84b3bd98-203c-44bb-aee8-9671995dbcb2.png />

</div>

---

# Introduction (cont.)
## Aren’t Existing Solutions Good Enough ?

<div class="grid grid-cols-7">

<p class="col-span-3">

Nevertheless, techniques prior to LoRA often introduce
- inference latency by extending model depth,
- reduce the model's usable sequence length.

more importantly, these methods often fail to match the fine-tuning baselines, posing a trade-off between efficiency and model quality.

</p>

<p class="col-span-4">

<img  src=https://user-images.githubusercontent.com/26186289/236154480-092797a2-c96f-4ce8-9667-4aa90f8f8c47.png />

<p class="pl-8">
When the generation length is shorter and the batch is smaller, the additional computational cost of the Adapter Layer is more obvious.
</p>

</p>

</div>

---


## **L**ow **R**ank **A**daptive

<img class="w-1/2" src=https://user-images.githubusercontent.com/26186289/236153734-06cb3e73-7800-44e8-8d1e-353a0c822c5b.png />

fine-tuning = $\Phi_0+\Delta\Phi$, 其中 $\Phi_0, \Delta\Phi \in \mathbb{R}^{d\times d}$
LoRA 將 $\Delta\Phi$ 分解成  $\Delta\Phi=\frac{\alpha}{r}AB^{\top}$, 其中 $AB \in \mathbb{R}^{d\times r}$, 且 $r\ll d$，
因此 LoRA 的可訓練參數遠比 full fine-tuning 更少，
並且在 inference 時可以將 $\Phi_0$ 與 $AB^{\top}$ 事先相加，就不會增加推理消耗的運算量

---

## Experiment

<img  src=https://user-images.githubusercontent.com/26186289/236169411-4b67768a-b68b-44fa-8a15-640c20d69f92.png />

---

<img  src=https://user-images.githubusercontent.com/26186289/236169552-e1f7c498-3a98-4627-b1f6-1e359f55acdb.png />

---

<img  src=https://user-images.githubusercontent.com/26186289/236169641-446eb8c5-b13a-42b9-93d7-51858befe37d.png />

---

![image](https://user-images.githubusercontent.com/26186289/236190473-4d3ce912-a4a7-4ea9-9f7d-5659cdd5f0d0.png)

